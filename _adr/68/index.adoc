---
num: 68
title: "Colocating managed services on the same data plane OSD fleet"
status: "Draft"
authors:
  - "Keith Wall"
  - "Tom Bentley"
  - "David Ffrench"
---

## Context and Problem Statement

As Red Hat starts to offer additional managed services, a portion of services will follow the bin-packed data plane deployment model which is used in RHOSAK. Each new data plane OSD cluster added to the fleet requires 3 master nodes and 3 infra nodes for a multi-az cluster. If each new bin-packed managed service deploys their own data plane OSD fleet, Red Hat's underlying infrastructure cost will be higher.

If Red Hat does not address this problem, the implication is the cost to customers of new managed services will be higher due to the additional master and infra nodes causing an increase in data plane infrastructure cost.

## Goals
* Identify how different managed services can be co-located on the same data plane fleet
* Outline network and capacity isolation between managed services

## Non-goals
* How installation of managed services will be initiated on the data plane fleet
* Outline how different SRE teams manage separate managed services on the same data plane fleet
* Determine the ideal node type (e.g. m5.xlarge) for new managed services
* Outline multi-az deployment model of new managed services

## Stakeholders
* Eng
* BU
* SRE

## Current Architecture

* We don’t currently have a second managed service with a deployment model which requires a data plane OSD cluster fleet.

* RHOSAK's data plane OSD clusters are created manually and registered with kas-fleet-manager which then installs all the components required such as the fleet-shard-operator, Strimzi operator and observability operator. Only once these components have installed successfully is the data plane OSD cluster ready to start assigning Kafka instances.
* All Kafka instances are scheduled to the compute nodes on the OSD clusters, there are no taints or tolerations currently in place. 

## Proposed Architecture

### Capacity Isolation

A new machine pool will be created for each new managed service. A machine pool is a group of worker nodes in a cluster that have the same configuration, providing ease of management.

This allows each new managed service to determine their own instance type that suits their needs. For example, M5 (general purpose), C5 (CPU optimised), R5 (Memory optimised) and at the size they would like.

All machine pools will be created through the OpenShift Cluster Manager (OCM) API, not directly on the OpenShift Dedicated (OSD) clusters. Each machine pool will be created with a `NoSchedule` taint which will require all pods scheduled for that managed service to include the toleration.

While outside the scope of this ADR, it is worth noting that with a shared data plane fleet, a new micro service in the control plane would be ideal to control scaling machine pools, creation and deletion of data plane OSD clusters.

### Network Isolation

Following the model used for RHOSAK, each new managed service should create their own dedicated IngressController/s on the data plane OSD cluster.

The kas-fleetshard-operator creates 4 new IngressControllers on each data plane OSD clusters as this is what works best for Kafka in a multi-az OSD cluster to limit the amount of cross availability zone traffic. It is up to each new managed service to determine how many IngressControllers and of what type, for example, external AWS NLB.

### Threat Model

* <Provide a link to the relevant threat model. You must either update an existing threat model(s) to cover the changes made by this ADR, or add a new threat model.

* Take a copy of the threat model when the ADR is proposed, and merge those changes in once the ADR is accepted

## Alternatives Considered / Rejected
### Shared Data Plane with no Capacity Isolation

Scheduling all managed services to the same machine pool has two downsides:

* Individual managed services don’t have the option to choose the node type that best suits their workloads 
Distinct slack capacity per managed service would not be possible. For example, RHOSAK wants to always ensure there is extra capacity to schedule 5 Kafka instances. With a shared machine pool, the slack capacity would also be shared.

* Scaling the nodes up and down becomes significantly more complex as the logic needs to consider all managed services.

### Separate Data Plane 

A separate data plane fleet for each managed service would require 6 additional nodes (3 master and 3 infra) for each OSD cluster. The significant cost impact here is for regions which are supported but rarely used across all managed services. With a separate data plane fleet, each new managed service would require at least one of their own OSD clusters in all supported regions.

## Challenges

Currently, all terraforming of a data plane RHOSAK cluster is performed through kas-fleet-manager. This logic should be split into its own microservice which will allow for supporting additional use cases such as the proposal here.

## Dependencies

* None

## Consequences if Not Completed

* Inconsistency across Red Hat in our approach to managed services deployment

* Cost ineffectiveness with additional OSD clusters for each managed service


